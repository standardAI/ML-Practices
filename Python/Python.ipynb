{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concurrency with Joblib\n",
    "# When dealing with large datasets or extensive computations, parallelizing tasks can significantly speed up \n",
    "# the process. Joblib is a library in Python that provides easy-to-use parallelism, allowing you to leverage all CPU cores.\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Define a function you want to run in parallel\n",
    "def compute(x):\n",
    "    return x * x\n",
    "\n",
    "# Create a list of inputs\n",
    "inputs = range(100000000)\n",
    "# Use Joblib's Parallel and delayed to run the compute function in parallel over all inputs\n",
    "results = Parallel(n_jobs=-1)(delayed(compute)(i) for i in inputs)\n",
    "# 'n_jobs=-1' will use all available CPU cores\n",
    "# 'results' will now contain the square of each number in 'inputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduling jobs\n",
    "# The schedule library in Python is used for running jobs at regular intervals. It's a lighter alternative to more complex task scheduling\n",
    "# libraries and tools, and it's often used for small-to-medium-sized projects where tasks need to run periodically but don't require the \n",
    "# full power of something like Cron jobs in Unix or Task Scheduler in Windows.\n",
    "# First, you'll need to install the schedule package if you haven't already. You can install it using pip:\n",
    "\n",
    "import schedule\n",
    "import time\n",
    "\n",
    "def my_job():\n",
    "    print(\"Doing the task...\")\n",
    "\n",
    "# Schedule the job to run every day at 8 am\n",
    "schedule.every().day.at(\"08:00\").do(my_job)\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38428/2864254135.py:13: size=35.0 MiB, count=999744, average=37 B\n",
      "/home/cosmos/mambaforge/envs/th/lib/python3.8/site-packages/traitlets/traitlets.py:676: size=1112 B, count=1, average=1112 B\n",
      "/home/cosmos/mambaforge/envs/th/lib/python3.8/site-packages/IPython/core/history.py:851: size=480 B, count=1, average=480 B\n",
      "/tmp/ipykernel_38428/2864254135.py:16: size=424 B, count=1, average=424 B\n",
      "/home/cosmos/mambaforge/envs/th/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3505: size=400 B, count=1, average=400 B\n"
     ]
    }
   ],
   "source": [
    "# Memory Profiling\n",
    "# Managing memory usage is critical, especially when dealing with large datasets or complex models. Python's tracemalloc module allows \n",
    "# users to trace memory blocks allocated by Python. It can be a lifesaver when debugging memory leaks or just for understanding where most \n",
    "# of the memory is being consumed.\n",
    "# Here's how you can utilize it:\n",
    "\n",
    "import tracemalloc\n",
    "\n",
    "# Start tracing memory allocations\n",
    "tracemalloc.start()\n",
    "\n",
    "# Your code that might be memory-intensive\n",
    "x = [i for i in range(1000000)]\n",
    "\n",
    "# Capture the current snapshot and display the top 5 memory-consuming lines\n",
    "snapshot = tracemalloc.take_snapshot()\n",
    "top_stats = snapshot.statistics('lineno')  # or 'filename' or 'traceback'\n",
    "\n",
    "# Display the top 5 lines consuming memory\n",
    "for stat in top_stats[:5]:\n",
    "    print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point(x=1, y=2)\n"
     ]
    }
   ],
   "source": [
    "# Parallelism\n",
    "# Python's “dataclasses” module offers a decorator and functions for auto-generating special methods in classes. With it, you can swiftly create\n",
    "# classes that primarily exist to hold values, eliminating the need to manually write boilerplate code like “__init__”.\n",
    "# In the code below, the “@dataclass” decorator automatically adds special methods to the class, including a well-defined “__init__” method. \n",
    "# This approach provides a concise and readable way to define classes that serve as data containers.\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    x: int\n",
    "    y: int\n",
    "\n",
    "p = Point(1, 2)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0 ran in process 38530\n",
      "Task 1 ran in process 38531\n",
      "Task 2 ran in process 38532\n",
      "Task 3 ran in process 38533\n",
      "Task 4 ran in process 38530\n",
      "Task 5 ran in process 38531\n",
      "Task 6 ran in process 38530\n",
      "Task 7 ran in process 38531\n",
      "Task 8 ran in process 38532\n",
      "Task 9 ran in process 38530\n"
     ]
    }
   ],
   "source": [
    "# Parallelism\n",
    "# Running tasks in parallel can greatly enhance the efficiency of your Python programs, particularly when dealing with CPU-bound operations.\n",
    "# Python's built-in `concurrent.futures` module provides a high-level interface for asynchronously executing callables, with the ProcessPoolExecutor\n",
    "# class facilitating parallelism.\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import os\n",
    "\n",
    "def task(n):\n",
    "    return (n, os.getpid())  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(executor.map(task, range(10)))\n",
    "\n",
    "    for n, pid in results:\n",
    "        print(f\"Task {n} ran in process {pid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List is empty\n"
     ]
    }
   ],
   "source": [
    "# Empty List\n",
    "# A straightforward way to check if a list is empty in Python is to take advantage of Python's ability to interpret empty collections (like lists)\n",
    "# as “False” in a boolean context, as shown in the code snippet below.\n",
    "# In provided example, an empty list will be evaluated as “False\", so “not x” will be “True” if the list “x” is empty. This is a clean and efficient \n",
    "# way to check for an empty list and is considered a good practice in Python.\n",
    "\n",
    "# Note that this will also work with other collections (like tuples, sets, and dictionaries) and strings, because Python treats empty collections and \n",
    "# strings as “False” when converting them to a boolean context.\n",
    "\n",
    "x = []\n",
    "if not x:\n",
    "    print(\"List is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 smallest values: [1, 5, 12]\n",
      "2 smallest values: [1, 5]\n",
      "5 largest values: [93, 53, 17, 12, 5]\n",
      "3 largest values: [93, 53]\n"
     ]
    }
   ],
   "source": [
    "# Heap Queue\n",
    "# The heap structure is used to represent priority queues and comes with the heapq module. Why is this useful? If you need to find the \n",
    "# n-largest or n-smallest elements from a list, this could be done by employing its two methods: nsmallest() and nlargest().\n",
    "\n",
    "import heapq\n",
    "\n",
    "simple_list = [5, 12, 93, 1, 53, 17]\n",
    "\n",
    "print('3 smallest values:', heapq.nsmallest(3, simple_list))\n",
    "print('2 smallest values:', heapq.nsmallest(2, simple_list))\n",
    "\n",
    "print('5 largest values:', heapq.nlargest(5, simple_list))\n",
    "print('3 largest values:', heapq.nlargest(2, simple_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: 0\n",
      "Saved: 2\n",
      "Saved: 4\n",
      "Saved: 6\n",
      "Saved: 8\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSaved: \u001b[39m\u001b[39m{\u001b[39;00mitem\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m pipeline \u001b[39m=\u001b[39m save_data(process_data(read_data()))\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m pipeline:\n\u001b[1;32m     22\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Generator Pipelines\n",
    "# In Python, generators are a powerful way to handle large streams of data efficiently.\n",
    "# An advanced technique is to pipeline multiple generators together to create a data processing workflow.\n",
    "# Each generator function processes data and yields an output, which becomes the input for the next generator in the pipeline.\n",
    "# Generator pipelines come in handy when you require handling large data streams with minimal memory overhead \n",
    "# you need to apply a sequence of transformations lazily creating modular and composable data processing workflows.\n",
    "\n",
    "def read_data():\n",
    "    for i in range(5):\n",
    "        yield i\n",
    "\n",
    "def process_data(data):\n",
    "    for item in data:\n",
    "        yield item * 2\n",
    "\n",
    "def save_data(data):\n",
    "    for item in data:\n",
    "        print(f\"Saved: {item}\")\n",
    "\n",
    "pipeline = save_data(process_data(read_data()))\n",
    "for _ in pipeline:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9227465\n",
      "Took 2.53030 seconds for fib without LRU.\n",
      "9227465\n",
      "Took 0.00008 seconds for fib with LRU.\n"
     ]
    }
   ],
   "source": [
    "# Caching with LRU Cache\n",
    "# Caching can boost Python code's performance, especially for functions called multiple times with the same arguments.\n",
    "# One way to implement this is with the lru_cache decorator from the functools module, where the \"LRU\" stands for Least Recently Used.\n",
    "# It caches the results of recent function calls, so re-calling with the same arguments returns the cached result.\n",
    "# When to Use LRU Cache:\n",
    "# For expensive or I/O-bound functions.\n",
    "# To improve the performance of recursive functions.\n",
    "# Here's a concise example using `lru_cache` to optimize a recursive Fibonacci function. \n",
    "# You can run this code snippet using OpenAI’s Code Interpreter, where it takes 0.00015 seconds to execute this script, \n",
    "# compared to 4.33 seconds for the non-cached Fibonacci function!\n",
    "\n",
    "from functools import lru_cache\n",
    "import time\n",
    "\n",
    "def fib(n):\n",
    "    return n if n <= 1 else fib(n-1) + fib(n-2)\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def fib_lru(n):\n",
    "    return n if n <= 1 else fib_lru(n-1) + fib_lru(n-2)\n",
    "\n",
    "start = time.time()\n",
    "print(fib(35))\n",
    "print(f\"Took {time.time() - start:.5f} seconds for fib without LRU.\")\n",
    "\n",
    "start = time.time()\n",
    "print(fib_lru(35))\n",
    "print(f\"Took {time.time() - start:.5f} seconds for fib with LRU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "\n",
    "- news@alphasignal.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "th",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
