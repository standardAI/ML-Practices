{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481 ms ± 20.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for _ in range(100):\n",
    "    # Creating on the CPU, then transfering to the GPU\n",
    "    cpu_tensor = torch.ones(1000, 64, 64)\n",
    "    gpu_tensor = cpu_tensor.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 ms ± 640 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for _ in range(100):\n",
    "    # Creating on the GPU\n",
    "    gpu_tensor = torch.ones((1000, 64, 64), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Sequential layers when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.mid_layers = []\n",
    "for _ in range(5):\n",
    "    self.mid_layers.append(nn.Linear(64, 64))\n",
    "    self.mid_layers.append(nn.ReLU())\n",
    "self.mid_layers = nn.Sequential(*self.mid_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for batch in data_batches:\n",
    "    output = example_model(batch)\n",
    "    target = torch.rand((10, 3))\n",
    "    loss = criterion(output, target)\n",
    "    losses.append(loss.detach())  # Or loss.item() if you don't need the gradient\n",
    "\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trick to delete a model from GPU\n",
    "example_model = ExampleModel().cuda()\n",
    "\n",
    "del example_model\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# Add from fastai notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed precision training uses both 16-bit and 32-bit floating-point types in a model during training to make it run faster\n",
    "# and use less memory. By keeping certain parts of the model in the 32-bit types for numerical stability, the model will have a \n",
    "# lower step time and train equally as well, with minor changes to the code.\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "model = YourModel().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scaler = GradScaler()\n",
    "\n",
    "for inputs, targets in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    with autocast():\n",
    "        loss = criterion(model(inputs), targets)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy Loading\n",
    "# Lazy loading is a design approach frequently used in machine learning and data-heavy applications, particularly when the full \n",
    "# dataset is too large to fit into memory. \n",
    "# This technique involves loading data into memory only as it's required during the execution of the program, rather than importing the \n",
    "# entire dataset all at once.\n",
    "# Use cases for lazy loading:\n",
    "#    Large Datasets: When the dataset is too large to fit into memory.\n",
    "#    Dynamic Data: When the dataset is continuously updated or changes over time.\n",
    "#    Random Access: When random (or pseudo-random) access to data points is acceptable, which is often the case in stochastic gradient descent algorithms.\n",
    "#    Streamed Data: When data can be streamed from distributed file systems, databases, or even online sources.\n",
    "#    Memory Efficiency: When you want to optimize the program to use as little memory as possible.\n",
    "# In PyTorch, lazy loading can be achieved with the use of the DataLoader class\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # add whatever transformations you need\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(root='path/to/your/images', transform=transform)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Clipping\n",
    "# Gradient clipping is a technique to prevent the gradients from becoming too large, which can lead to exploding gradient problems especially \n",
    "# in recurrent neural networks (RNNs). This can be particularly useful when you are observing NaNs during training or when the losses go to infinity.\n",
    "\n",
    "# PyTorch provides a simple utility called torch.nn.utils.clip_grad_norm_ which can be used to clip the gradients of model parameters.\n",
    "# Here's how to use it:\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# define your model and optimizer\n",
    "model = YourModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for inputs, targets in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradients after computing the backward pass and before the optimization step\n",
    "    clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization\n",
    "# Quantization is a process used to reduce the computational and storage burdens of machine learning models by converting weights from floating \n",
    "# point representations to lower precision, such as int8 or int16. This has the benefit of reducing memory requirements and speeding up model \n",
    "# inference time, with a potential trade-off in model performance due to the reduced precision.\n",
    "\n",
    "# PyTorch's quantization API provides tools for post-training quantization (quantization after the model has been trained), quantization-aware\n",
    "# training (where quantization is considered during the training process itself), and dynamic quantization (only quantizes certain parts of the model, \n",
    "# typically the weights).\n",
    "# Here's a very simplified example of how to use PyTorch's dynamic quantization on a BERT model:\n",
    "\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Convert to torchscript and quantize model\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Save quantized model\n",
    "torch.jit.save(torch.jit.script(quantized_model), \"quantized_bert.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method named_parameters()\n",
    "\n",
    "# Instantiate a pre-trained model\n",
    "transfer_model = resnet50(pretrained=True)\n",
    "\n",
    "# Freeze the weights of the model\n",
    "for name, param in transfer_model.named_parameters():\n",
    "\tparam.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-Scale Batch Size\n",
    "# Auto-scaling the batch size is a technique to automatically find the largest batch size that fits into memory for a given model and data.\n",
    "# This is beneficial because larger batch sizes can result in faster training times due to more efficient use of hardware resources (GPU).\n",
    "# However, the downside is that they can also result in memory overflow errors if the batch size is too large for the GPU to handle.\n",
    "# Within PyTorch framework, TOMA is an approach that can be used for auto-scaling batch sizes. Specifically, it retries code that fails due to OOM \n",
    "# (out-of-memory) conditions and lowers batch sizes automatically (e.g. from 512, to 256, then 128, etc.). To avoid failing over repeatedly, a \n",
    "# simple cache is implemented that memorizes that last successful batchsize given the call and available free memory.\n",
    "# Link: https://github.com/BlackHC/toma \n",
    "\n",
    "!pip install toma\n",
    "\n",
    "from toma import toma\n",
    "\n",
    "@toma.batch(initial_batchsize=512)\n",
    "def run_inference(batchsize, model, dataset):\n",
    "\t# your inference code\n",
    "\n",
    "run_inference(batchsize, model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need explanation\n",
    "\n",
    "optimizer = ...\n",
    "NUM_ACCUMULATION_STEPS = ...\n",
    "for epoch in range(...):\n",
    "    for idx, sample in enumerate(dataloader):\n",
    "        inputs, labels = sample\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = model(inputs)\n",
    "        # Compute Loss and Perform Back-propagation\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Normalize the Gradients\n",
    "        loss = loss / NUM_ACCUMULATION_STEPS\n",
    "        loss.backward()\n",
    "        if (\n",
    "            ((idx + 1) % NUM_ACCUMULATION_STEPS == 0) \n",
    "            or (idx + 1 == len(dataloader))\n",
    "        ):\n",
    "            # Update Optimizer\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Hooks\n",
    "# Hooks in PyTorch allow you to modify or monitor the forward and backward passes in a neural network.\n",
    "# They are essentially callback functions that can be registered on nn.Module instances, including both layers and entire models.\n",
    "# These callbacks get executed when the forward or backward pass runs through the module. \n",
    "# Use cases:\n",
    "# Debugging: Inspect values within the network.\n",
    "# Gradient Clipping or Modification: Modify gradients during backpropagation, e.g., to prevent gradient explosion.\n",
    "# Feature Extraction: Extract the output of intermediate layers for analysis or other tasks like transfer learning.\n",
    "# Resource Optimization: Monitor resource usage dynamically during training.\n",
    "\n",
    "# Forward Hook\n",
    "def forward_hook(module, input, output):\n",
    "    print('Inside forward hook.')\n",
    "layer = nn.Linear(2, 2)\n",
    "hook1 = layer.register_forward_hook(forward_hook)\n",
    "\n",
    "# Backward Hook\n",
    "def backward_hook(module, grad_input, grad_output):\n",
    "    print('Inside backward hook.')\n",
    "hook2 = layer.register_backward_hook(backward_hook)\n",
    "\n",
    "# Removing Hooks\n",
    "hook1.remove()\n",
    "hook2.remove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "th",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:58:50) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0eba0d3721dd99d5e85d92095d30b3f3baa213223e21c8471968ea015153b8df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
