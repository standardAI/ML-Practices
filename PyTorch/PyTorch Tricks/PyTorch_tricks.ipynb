{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducible results\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    #torch.set_deterministic_debug_mode(True)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "    \n",
    "seed_everything(0)\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632 ms ± 4.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for _ in range(100):\n",
    "    # Creating on the CPU, then transfering to the GPU\n",
    "    cpu_tensor = torch.ones(1000, 64, 64)\n",
    "    gpu_tensor = cpu_tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.1 ms ± 899 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for _ in range(100):\n",
    "    # Creating on the GPU\n",
    "    gpu_tensor = torch.ones((1000, 64, 64), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616 ms ± 5.01 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for _ in range(100):\n",
    "    # Creating on the CPU, then converting to half precision\n",
    "    f32_tensor = torch.ones(1000, 64, 64)\n",
    "    f16_tensor = f32_tensor.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 ms ± 1.45 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for _ in range(100):\n",
    "    # Creating on the CPU in half precision\n",
    "    f16_tensor = torch.ones((1000, 64, 64), dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.5 ms ± 6.13 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for _ in range(100):\n",
    "    # Creating on the GPU, then converting to half precision\n",
    "    f32_tensor = torch.ones((1000, 64, 64), device=\"cuda\")\n",
    "    f16_tensor = f32_tensor.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.1 ms ± 911 ns per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for _ in range(100):\n",
    "    # Creating on the GPU in half precision\n",
    "    f16_tensor = torch.ones((1000, 64, 64), dtype=torch.float16, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Sequential layers when possible but debugging is harder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.mid_layers = []\n",
    "for _ in range(5):\n",
    "    self.mid_layers.append(nn.Linear(64, 64))\n",
    "    self.mid_layers.append(nn.ReLU())\n",
    "self.mid_layers = nn.Sequential(*self.mid_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for batch in data_batches:\n",
    "    output = example_model(batch)\n",
    "    target = torch.rand((10, 3))\n",
    "    loss = criterion(output, target)\n",
    "    losses.append(loss.detach())  # Or loss.item() if you don't need the gradient\n",
    "\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trick to delete a model from GPU\n",
    "example_model = ExampleModel().cuda()\n",
    "\n",
    "del example_model\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "# Added from fastai notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed precision training uses both 16-bit and 32-bit floating-point types in a model during training to make it run faster\n",
    "# and use less memory. By keeping certain parts of the model in the 32-bit types for numerical stability, the model will have a \n",
    "# lower step time and train equally as well, with minor changes to the code.\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "model = YourModel().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scaler = GradScaler()\n",
    "\n",
    "for inputs, targets in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    with autocast():\n",
    "        loss = criterion(model(inputs), targets)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy Loading\n",
    "# Lazy loading is a design approach frequently used in machine learning and data-heavy applications, particularly when the full \n",
    "# dataset is too large to fit into memory. \n",
    "# This technique involves loading data into memory only as it's required during the execution of the program, rather than importing the \n",
    "# entire dataset all at once.\n",
    "# Use cases for lazy loading:\n",
    "#    Large Datasets: When the dataset is too large to fit into memory.\n",
    "#    Dynamic Data: When the dataset is continuously updated or changes over time.\n",
    "#    Random Access: When random (or pseudo-random) access to data points is acceptable, which is often the case in stochastic gradient descent algorithms.\n",
    "#    Streamed Data: When data can be streamed from distributed file systems, databases, or even online sources.\n",
    "#    Memory Efficiency: When you want to optimize the program to use as little memory as possible.\n",
    "# In PyTorch, lazy loading can be achieved with the use of the DataLoader class\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # add whatever transformations you need\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(root='path/to/your/images', transform=transform)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Clipping\n",
    "# Gradient clipping is a technique to prevent the gradients from becoming too large, which can lead to exploding gradient problems especially \n",
    "# in recurrent neural networks (RNNs). This can be particularly useful when you are observing NaNs during training or when the losses go to infinity.\n",
    "\n",
    "# PyTorch provides a simple utility called torch.nn.utils.clip_grad_norm_ which can be used to clip the gradients of model parameters.\n",
    "# Here's how to use it:\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# define your model and optimizer\n",
    "model = YourModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for inputs, targets in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradients after computing the backward pass and before the optimization step\n",
    "    clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization\n",
    "# Quantization is a process used to reduce the computational and storage burdens of machine learning models by converting weights from floating \n",
    "# point representations to lower precision, such as int8 or int16. This has the benefit of reducing memory requirements and speeding up model \n",
    "# inference time, with a potential trade-off in model performance due to the reduced precision.\n",
    "\n",
    "# PyTorch's quantization API provides tools for post-training quantization (quantization after the model has been trained), quantization-aware\n",
    "# training (where quantization is considered during the training process itself), and dynamic quantization (only quantizes certain parts of the model, \n",
    "# typically the weights).\n",
    "# Here's a very simplified example of how to use PyTorch's dynamic quantization on a BERT model:\n",
    "\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Convert to torchscript and quantize model\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "# Save quantized model\n",
    "torch.jit.save(torch.jit.script(quantized_model), \"quantized_bert.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method named_parameters()\n",
    "\n",
    "# Instantiate a pre-trained model\n",
    "transfer_model = resnet50(pretrained=True)\n",
    "\n",
    "# Freeze the weights of the model\n",
    "for name, param in transfer_model.named_parameters():\n",
    "\tparam.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-Scale Batch Size\n",
    "# Auto-scaling the batch size is a technique to automatically find the largest batch size that fits into memory for a given model and data.\n",
    "# This is beneficial because larger batch sizes can result in faster training times due to more efficient use of hardware resources (GPU).\n",
    "# However, the downside is that they can also result in memory overflow errors if the batch size is too large for the GPU to handle.\n",
    "# Within PyTorch framework, TOMA is an approach that can be used for auto-scaling batch sizes. Specifically, it retries code that fails due to OOM \n",
    "# (out-of-memory) conditions and lowers batch sizes automatically (e.g. from 512, to 256, then 128, etc.). To avoid failing over repeatedly, a \n",
    "# simple cache is implemented that memorizes that last successful batchsize given the call and available free memory.\n",
    "# Link: https://github.com/BlackHC/toma \n",
    "\n",
    "!pip install toma\n",
    "\n",
    "from toma import toma\n",
    "\n",
    "@toma.batch(initial_batchsize=512)\n",
    "def run_inference(batchsize, model, dataset):\n",
    "\t# your inference code\n",
    "\n",
    "run_inference(batchsize, model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need explanation\n",
    "\n",
    "optimizer = ...\n",
    "NUM_ACCUMULATION_STEPS = ...\n",
    "for epoch in range(...):\n",
    "    for idx, sample in enumerate(dataloader):\n",
    "        inputs, labels = sample\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = model(inputs)\n",
    "        # Compute Loss and Perform Back-propagation\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Normalize the Gradients\n",
    "        loss = loss / NUM_ACCUMULATION_STEPS\n",
    "        loss.backward()\n",
    "        if (\n",
    "            ((idx + 1) % NUM_ACCUMULATION_STEPS == 0) \n",
    "            or (idx + 1 == len(dataloader))\n",
    "        ):\n",
    "            # Update Optimizer\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Hooks\n",
    "# Hooks in PyTorch allow you to modify or monitor the forward and backward passes in a neural network.\n",
    "# They are essentially callback functions that can be registered on nn.Module instances, including both layers and entire models.\n",
    "# These callbacks get executed when the forward or backward pass runs through the module. \n",
    "# Use cases:\n",
    "# Debugging: Inspect values within the network.\n",
    "# Gradient Clipping or Modification: Modify gradients during backpropagation, e.g., to prevent gradient explosion.\n",
    "# Feature Extraction: Extract the output of intermediate layers for analysis or other tasks like transfer learning.\n",
    "# Resource Optimization: Monitor resource usage dynamically during training.\n",
    "\n",
    "# Forward Hook\n",
    "def forward_hook(module, input, output):\n",
    "    print('Inside forward hook.')\n",
    "layer = nn.Linear(2, 2)\n",
    "hook1 = layer.register_forward_hook(forward_hook)\n",
    "\n",
    "# Backward Hook\n",
    "def backward_hook(module, grad_input, grad_output):\n",
    "    print('Inside backward hook.')\n",
    "hook2 = layer.register_backward_hook(backward_hook)\n",
    "\n",
    "# Removing Hooks\n",
    "hook1.remove()\n",
    "hook2.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3019, 0.6283, 0.5609, 0.7612, 0.6735])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(4, 8, 5)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 5])\n",
      "tensor([[[0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735]],\n",
      "\n",
      "        [[0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735]],\n",
      "\n",
      "        [[0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735]],\n",
      "\n",
      "        [[0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735],\n",
      "         [0.3019, 0.6283, 0.5609, 0.7612, 0.6735]]])\n"
     ]
    }
   ],
   "source": [
    "z = x.expand_as(y)\n",
    "print(z.shape, z, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch v0.2.0 relase\n",
    "\n",
    "#General Semantics\n",
    "\n",
    "#Two tensors are “broadcastable” if the following rules hold:\n",
    "\n",
    "#   Each tensor has at least one dimension.\n",
    "#   When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, \n",
    "#   one of them is 1, or one of them does not exist.\n",
    "\n",
    "#For Example:\n",
    "\n",
    "x=torch.FloatTensor(5,7,3)\n",
    "y=torch.FloatTensor(5,7,3)\n",
    "# same shapes are always broadcastable (i.e. the above rules always hold)\n",
    "\n",
    "# can line up trailing dimensions\n",
    "x=torch.FloatTensor(5,3,4,1)\n",
    "y=torch.FloatTensor(  3,1,1)\n",
    "\n",
    "# x and y are broadcastable.\n",
    "# 1st trailing dimension: both have size 1\n",
    "# 2nd trailing dimension: y has size 1\n",
    "# 3rd trailing dimension: x size == y size\n",
    "# 4th trailing dimension: y dimension doesn't exist\n",
    "\n",
    "# but:\n",
    "x=torch.FloatTensor(5,2,4,1)\n",
    "y=torch.FloatTensor(  3,1,1)\n",
    "# x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3\n",
    "\n",
    "torch.add(torch.ones(4,1), torch.randn(4))\n",
    "\n",
    "# would previously produce a Tensor with size: torch.Size([4,1]),\n",
    "# but now produces a Tensor with size: torch.Size([4,4])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 4, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If two tensors x, y are \"broadcastable\", the resulting tensor size is calculated as follows:\n",
    "\n",
    "#   If the number of dimensions of x and y are not equal, prepend 1 to the dimensions of the tensor with fewer dimensions to make them equal length.\n",
    "#   Then, for each dimension size, the resulting dimension size is the max of the sizes of x and y along that dimension.\n",
    "\n",
    "# For Example:\n",
    "\n",
    "# can line up trailing dimensions to make reading easier\n",
    "x=torch.FloatTensor(5,1,4,1)\n",
    "y=torch.FloatTensor(  3,1,1)\n",
    "(x+y).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/cosmos/Documents/Practices/ML-Practices/PyTorch/PyTorch Tricks/PyTorch_tricks.ipynb Cell 21\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cosmos/Documents/Practices/ML-Practices/PyTorch/PyTorch%20Tricks/PyTorch_tricks.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m x\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mFloatTensor(\u001b[39m5\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cosmos/Documents/Practices/ML-Practices/PyTorch/PyTorch%20Tricks/PyTorch_tricks.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mFloatTensor(  \u001b[39m3\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/cosmos/Documents/Practices/ML-Practices/PyTorch/PyTorch%20Tricks/PyTorch_tricks.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m (x\u001b[39m+\u001b[39;49my)\u001b[39m.\u001b[39msize()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/cosmos/Documents/Practices/ML-Practices/PyTorch/PyTorch%20Tricks/PyTorch_tricks.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# error case\n",
    "x=torch.FloatTensor(5,2,4,1)\n",
    "y=torch.FloatTensor(  3,1,1)\n",
    "(x+y).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(5, 5, 5)\n",
    "\n",
    "# Pure Integer Array Indexing - specify arbitrary indices at each dimension\n",
    "\n",
    "x[[1, 2], [3, 2], [1, 0]]\n",
    "#--> yields a 2-element Tensor (x[1][3][1], x[2][2][0])\n",
    "\n",
    "# also supports broadcasting, duplicates\n",
    "\n",
    "x[[2, 3, 2], [0], [1]]\n",
    "#--> yields a 3-element Tensor (x[2][0][1], x[3][0][1], x[2][0][1])\n",
    "\n",
    "# arbitrary indexer shapes allowed\n",
    "\n",
    "x[[[1, 0], [0, 1]], [0], [1]].shape\n",
    "#--> yields a 2x2 Tensor [[x[1][0][1], x[0][0][1]],\n",
    "#                         [x[0][0][1], x[1][0][1]]]\n",
    "\n",
    "# can use colon, ellipse\n",
    "\n",
    "x[[0, 3], :, :]\n",
    "x[[0, 3], ...]\n",
    "#--> both yield a 2x5x5 Tensor [x[0], x[3]]\n",
    "\n",
    "# also use Tensors to index!\n",
    "\n",
    "y = torch.LongTensor([0, 2, 4])\n",
    "x[y, :, :]\n",
    "#--> yields a 3x5x5 Tensor [x[0], x[2], x[4]]\n",
    "\n",
    "# selection with less than ndim, note the use of comma\n",
    "\n",
    "x[[1, 3], ]\n",
    "#--> yields a 2x5x5 Tensor [x[1], x[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed PyTorch\n",
    "\n",
    "# Wrap model in DistributedDataParallel (CUDA only for the moment)\n",
    "model = torch.nn.parallel.DistributedDataParallel(model.cuda())\n",
    "\n",
    "# Use a DistributedSampler to restrict each process to a distinct subset\n",
    "# of the dataset.\n",
    "train_dataset = ...\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=args.batch_size, num_workers=args.workers,\n",
    "    pin_memory=True, sampler=train_sampler)\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    # Use .set_epoch() method to reshuffle the dataset partition at every iteration\n",
    "    train_sampler.set_epoch(epoch)\n",
    "    # training loop\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerically stable Binary Cross-Entropy loss via bce_with_logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorc 0.3.0\n",
    "\"\"\"\n",
    "Unreduced losses\n",
    "\n",
    "Now, Some loss functions can compute per-sample losses in a mini-batch\n",
    "\n",
    "    By default PyTorch sums losses over the mini-batch and returns a single scalar loss. This was limiting to users.\n",
    "    Now, a subset of loss functions allow specifying reduce=False to return individual losses for each sample in the mini-batch\n",
    "    Example: loss = nn.CrossEntropyLoss(..., reduce=False)\n",
    "    Currently supported losses: MSELoss, NLLLoss, NLLLoss2d, KLDivLoss, CrossEntropyLoss, SmoothL1Loss, L1Loss\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "      autograd::engine::evaluate_function: PowBackward0         3.97%      15.000us        49.74%     188.000us     188.000us             1  \n",
      "                                           PowBackward0         6.88%      26.000us        45.77%     173.000us     173.000us             1  \n",
      "                                              aten::pow        37.04%     140.000us        43.92%     166.000us      83.000us             2  \n",
      "                                              aten::mul        13.23%      50.000us        20.63%      78.000us      39.000us             2  \n",
      "                                        aten::ones_like         3.70%      14.000us        14.55%      55.000us      55.000us             1  \n",
      "                                       aten::empty_like         3.70%      14.000us        10.05%      38.000us      38.000us             1  \n",
      "autograd::engine::evaluate_function: torch::autograd...         2.12%       8.000us        10.05%      38.000us      38.000us             1  \n",
      "                                               aten::to         2.12%       8.000us         7.94%      30.000us      10.000us             3  \n",
      "                        torch::autograd::AccumulateGrad         3.44%      13.000us         7.94%      30.000us      30.000us             1  \n",
      "                                    aten::empty_strided         7.67%      29.000us         7.67%      29.000us      14.500us             2  \n",
      "                                            aten::copy_         5.82%      22.000us         5.82%      22.000us      11.000us             2  \n",
      "                                         aten::_to_copy         3.17%      12.000us         5.82%      22.000us      22.000us             1  \n",
      "                                           aten::detach         1.32%       5.000us         4.50%      17.000us      17.000us             1  \n",
      "                                                 detach         3.17%      12.000us         3.17%      12.000us      12.000us             1  \n",
      "                                      aten::result_type         1.85%       7.000us         1.85%       7.000us       3.500us             2  \n",
      "                                            aten::fill_         0.79%       3.000us         0.79%       3.000us       3.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 378.000us\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-10-26 16:49:51 33993:33993 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2023-10-26 16:49:51 33993:33993 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2023-10-26 16:49:51 33993:33993 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((1, 1), requires_grad=True)\n",
    "\n",
    "with torch.autograd.profiler.profile() as prof:\n",
    "    y = x ** 2\n",
    "    y.backward()\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18()\n",
    "x = torch.randn(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==36048== NVPROF is profiling process 36048, command: python model_test.py\n",
      "==36048== Generated result file: /home/cosmos/Documents/Practices/ML-Practices/PyTorch/PyTorch Tricks/trace_name.prof\n"
     ]
    }
   ],
   "source": [
    "!nvprof --profile-from-start off -o trace_name.prof -- python model_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Expected time_us == 0 but got 129954",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/cosmos/Documents/Practices/ML-Practices/PyTorch/PyTorch Tricks/PyTorch_tricks.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cosmos/Documents/Practices/ML-Practices/PyTorch/PyTorch%20Tricks/PyTorch_tricks.ipynb#X35sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Then, you can load trace_name.prof in PyTorch and print a summary profile report.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/cosmos/Documents/Practices/ML-Practices/PyTorch/PyTorch%20Tricks/PyTorch_tricks.ipynb#X35sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m prof \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mload_nvprof(\u001b[39m'\u001b[39m\u001b[39mtrace_name.prof\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/cosmos/Documents/Practices/ML-Practices/PyTorch/PyTorch%20Tricks/PyTorch_tricks.ipynb#X35sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39;49m(prof)\n",
      "File \u001b[0;32m~/miniforge3/envs/th/lib/python3.9/site-packages/torch/autograd/profiler_util.py:47\u001b[0m, in \u001b[0;36mEventList.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__str__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtable()\n",
      "File \u001b[0;32m~/miniforge3/envs/th/lib/python3.9/site-packages/torch/autograd/profiler_util.py:196\u001b[0m, in \u001b[0;36mEventList.table\u001b[0;34m(self, sort_by, row_limit, max_src_column_width, max_name_column_width, max_shapes_column_width, header, top_level_events_only)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtable\u001b[39m(\n\u001b[1;32m    170\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    171\u001b[0m     sort_by\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     top_level_events_only\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m ):\n\u001b[1;32m    179\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Prints an EventList as a nicely formatted table.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39m        A string containing the table.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mreturn\u001b[39;00m _build_table(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    198\u001b[0m         sort_by\u001b[39m=\u001b[39;49msort_by,\n\u001b[1;32m    199\u001b[0m         row_limit\u001b[39m=\u001b[39;49mrow_limit,\n\u001b[1;32m    200\u001b[0m         max_src_column_width\u001b[39m=\u001b[39;49mmax_src_column_width,\n\u001b[1;32m    201\u001b[0m         max_name_column_width\u001b[39m=\u001b[39;49mmax_name_column_width,\n\u001b[1;32m    202\u001b[0m         max_shapes_column_width\u001b[39m=\u001b[39;49mmax_shapes_column_width,\n\u001b[1;32m    203\u001b[0m         header\u001b[39m=\u001b[39;49mheader,\n\u001b[1;32m    204\u001b[0m         profile_memory\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_profile_memory,\n\u001b[1;32m    205\u001b[0m         with_flops\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_flops,\n\u001b[1;32m    206\u001b[0m         top_level_events_only\u001b[39m=\u001b[39;49mtop_level_events_only,\n\u001b[1;32m    207\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/th/lib/python3.9/site-packages/torch/autograd/profiler_util.py:1087\u001b[0m, in \u001b[0;36m_build_table\u001b[0;34m(events, sort_by, header, row_limit, max_src_column_width, max_name_column_width, max_shapes_column_width, with_flops, profile_memory, top_level_events_only)\u001b[0m\n\u001b[1;32m   1070\u001b[0m row_values \u001b[39m=\u001b[39m [\n\u001b[1;32m   1071\u001b[0m     name,\n\u001b[1;32m   1072\u001b[0m     \u001b[39m# Self CPU total %, 0 for async events.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1080\u001b[0m     evt\u001b[39m.\u001b[39mcpu_time_str,  \u001b[39m# CPU time avg\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m ]\n\u001b[1;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m has_cuda_time:\n\u001b[1;32m   1083\u001b[0m     row_values\u001b[39m.\u001b[39mextend(\n\u001b[1;32m   1084\u001b[0m         [\n\u001b[1;32m   1085\u001b[0m             evt\u001b[39m.\u001b[39mself_cuda_time_total_str,\n\u001b[1;32m   1086\u001b[0m             \u001b[39m# CUDA time total %\u001b[39;00m\n\u001b[0;32m-> 1087\u001b[0m             _format_time_share(\n\u001b[1;32m   1088\u001b[0m                 evt\u001b[39m.\u001b[39;49mself_cuda_time_total, sum_self_cuda_time_total\n\u001b[1;32m   1089\u001b[0m             ),\n\u001b[1;32m   1090\u001b[0m             evt\u001b[39m.\u001b[39mcuda_time_total_str,\n\u001b[1;32m   1091\u001b[0m             evt\u001b[39m.\u001b[39mcuda_time_str,  \u001b[39m# Cuda time avg\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m         ]\n\u001b[1;32m   1093\u001b[0m     )\n\u001b[1;32m   1094\u001b[0m \u001b[39mif\u001b[39;00m has_privateuse1_time:\n\u001b[1;32m   1095\u001b[0m     row_values\u001b[39m.\u001b[39mextend(\n\u001b[1;32m   1096\u001b[0m         [\n\u001b[1;32m   1097\u001b[0m             evt\u001b[39m.\u001b[39mself_privateuse1_time_total_str,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         ]\n\u001b[1;32m   1105\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/th/lib/python3.9/site-packages/torch/autograd/profiler_util.py:367\u001b[0m, in \u001b[0;36m_format_time_share\u001b[0;34m(time_us, total_time_us)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Defines how to format time in FunctionEvent\"\"\"\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m total_time_us \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 367\u001b[0m     \u001b[39massert\u001b[39;00m time_us \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected time_us == 0 but got \u001b[39m\u001b[39m{\u001b[39;00mtime_us\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNaN\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtime_us\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39m\u001b[39m100.0\u001b[39m\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39mtotal_time_us\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected time_us == 0 but got 129954"
     ]
    }
   ],
   "source": [
    "# The profiler works for both CPU and CUDA models.\n",
    "# For CUDA models, you have to run your python program with a special nvprof prefix. For example:\n",
    "\n",
    "# nvprof --profile-from-start off -o trace_name.prof -- python <your arguments>\n",
    "\"\"\"\n",
    "# in python\n",
    "with torch.cuda.profiler.profile():\n",
    "    model(x) # Warmup CUDA memory allocator and profiler\n",
    "    with torch.autograd.profiler.emit_nvtx():\n",
    "        model(x)\n",
    "\"\"\"\n",
    "# Then, you can load trace_name.prof in PyTorch and print a summary profile report.\n",
    "\n",
    "prof = torch.autograd.profiler.load_nvprof('trace_name.prof')\n",
    "print(prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-10-27 18:12:48 26895:26895 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                      aten::empty         0.46%       1.432ms         4.10%      12.882ms      64.410us      94.86 Mb      94.85 Mb           200  \n",
      "    aten::max_pool2d_with_indices         4.91%      15.424ms         4.91%      15.424ms      15.424ms      11.48 Mb      11.48 Mb             1  \n",
      "                      aten::addmm         2.91%       9.130ms         2.92%       9.172ms       9.172ms      19.53 Kb      19.53 Kb             1  \n",
      "                       aten::mean         0.05%     172.000us         0.11%     340.000us     340.000us      10.00 Kb      10.00 Kb             1  \n",
      "                      aten::fill_         3.65%      11.464ms         3.65%      11.464ms      56.752us       2.00 Kb       2.00 Kb           202  \n",
      "              aten::empty_strided         0.00%       9.000us         0.00%       9.000us       9.000us           4 b           4 b             1  \n",
      "                     aten::conv2d         0.50%       1.576ms        82.15%     258.097ms      12.905ms      47.37 Mb           0 b            20  \n",
      "                aten::convolution         0.18%     580.000us        81.65%     256.521ms      12.826ms      47.37 Mb           0 b            20  \n",
      "               aten::_convolution         0.52%       1.626ms        81.47%     255.941ms      12.797ms      47.37 Mb           0 b            20  \n",
      "         aten::mkldnn_convolution        78.75%     247.398ms        80.95%     254.315ms      12.716ms      47.37 Mb           0 b            20  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 314.166ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-10-27 18:12:49 26895:26895 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2023-10-27 18:12:49 26895:26895 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "model = models.resnet18()\n",
    "inputs = torch.randn(5, 3, 224, 224)\n",
    "with profiler.profile(profile_memory=True, record_shapes=True) as prof:\n",
    "    model(inputs)\n",
    "\n",
    "# NOTE: some columns were removed for brevity\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n",
    "# ---------------------------  ---------------  ---------------  ---------------\n",
    "# Name                         CPU Mem          Self CPU Mem     Number of Calls\n",
    "# ---------------------------  ---------------  ---------------  ---------------\n",
    "# empty                        94.79 Mb         94.79 Mb         123\n",
    "# resize_                      11.48 Mb         11.48 Mb         2\n",
    "# addmm                        19.53 Kb         19.53 Kb         1\n",
    "# empty_strided                4 b              4 b              1\n",
    "# conv2d                       47.37 Mb         0 b              20\n",
    "# ---------------------------  ---------------  ---------------  ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Optimizers\n",
    "\n",
    "    optim.SparseAdam: Implements a lazy version of Adam algorithm suitable for sparse tensors.\n",
    "        In this variant, only moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters.\n",
    "    Optimizers now have an add_param_group function that lets you add new parameter groups to an already constructed optimizer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1650\n",
      "(7, 5)\n",
      "_CudaDeviceProperties(name='NVIDIA GeForce GTX 1650', major=7, minor=5, total_memory=3903MB, multi_processor_count=16)\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0), torch.cuda.get_device_capability(0), torch.cuda.get_device_properties(0), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pad_packed_sequence now allows a padding_value argument that can be used instead of zero-padding\n",
    "\n",
    "Dataset now has a + operator (which uses ConcatDataset). You can do something like MNIST(...) + FashionMNIST(...) \n",
    "for example, and you will get a concatenated dataset containing samples from both.\n",
    "\n",
    "If you want to load a model's state_dict into another model (for example to fine-tune a pre-trained network), \n",
    "load_state_dict was strict on matching the key names of the parameters. Now we provide a strict=False option to \n",
    "load_state_dict where it only loads in parameters where the keys match, and ignores the other parameter keys.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.1'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 0.3.1 release\n",
    "\n",
    "# Allow map_location in torch.load to be a string, such as map_location='cpu' or map_location='cuda:2' #4203\n",
    "model = torch.load('model.pt', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.DoubleTensor\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.DoubleTensor([1, 1, 1])\n",
    "print(type(x))\n",
    "print(x.type())\n",
    "print(isinstance(x, torch.DoubleTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What about .data?\n",
    "\n",
    ".data was the primary way to get the underlying Tensor from a Variable. After this merge, calling y = x.data still has similar semantics.\n",
    "So y will be a Tensor that shares the same data with x, is unrelated with the computation history of x, and has requires_grad=False.\n",
    "\n",
    "However, .data can be unsafe in some cases. Any changes on x.data wouldn't be tracked by autograd, and the computed gradients would be \n",
    "incorrect if x is needed in a backward pass. A safer alternative is to use x.detach(), which also returns a Tensor that shares data with \n",
    "requires_grad=False, but will have its in-place changes reported by autograd if x is needed in backward.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.1416)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(3.1416)         # create a scalar directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(3.1416).size()  # scalar is 0-dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([3]).size()     # compare to a vector of size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4, 5])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = torch.arange(2, 6)  # this is a vector\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector[3]                    # indexing into a vector gives a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector[3].item()             # .item() gives the value as a Python number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = torch.tensor([2, 3]).sum()\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(1, requires_grad=True)\n",
    "y = x * 2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(1, requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x * 2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(1, requires_grad=True)\n",
    "with torch.inference_mode():\n",
    "    y = x * 2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(1, requires_grad=True)\n",
    "is_train = False\n",
    "with torch.set_grad_enabled(is_train):\n",
    "    y = x * 2\n",
    "y.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float16,\n",
       " torch.bfloat16,\n",
       " torch.float32,\n",
       " 4,\n",
       " tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.uint8),\n",
       " 4)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_autocast_gpu_dtype(), torch.get_autocast_cpu_dtype(), torch.get_default_dtype(), torch.get_num_threads(), torch.get_rng_state(), torch.get_num_interop_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below is a complete list of available torch.dtypes (data types) and their corresponding tensor types.\n",
    "Data type \t                torch.dtype \t                Tensor types\n",
    "32-bit floating point \t    torch.float32 or torch.float \ttorch.*.FloatTensor\n",
    "64-bit floating point \t    torch.float64 or torch.double \ttorch.*.DoubleTensor\n",
    "16-bit floating point \t    torch.float16 or torch.half \ttorch.*.HalfTensor\n",
    "8-bit integer (unsigned) \ttorch.uint8 \t                torch.*.ByteTensor\n",
    "8-bit integer (signed) \t    torch.int8 \t                    torch.*.CharTensor\n",
    "16-bit integer (signed) \ttorch.int16 or torch.short \t    torch.*.ShortTensor\n",
    "32-bit integer (signed) \ttorch.int32 or torch.int \t    torch.*.IntTensor\n",
    "64-bit integer (signed) \ttorch.int64 or torch.long \t    torch.*.LongTensor\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_id = torch.cuda.current_device()\n",
    "\n",
    "torch.device('{device_type}:{device_ordinal}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, dtype=torch.float64)\n",
    "x.new_ones(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1], dtype=torch.int32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.new_ones(4, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1416, 3.1416, 3.1416],\n",
       "        [3.1416, 3.1416, 3.1416]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((2, 3), 3.1416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 0, 1])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "torch.empty \t    unintialized memory\n",
    "torch.zeros \t    all zeros\n",
    "torch.ones \t        all ones\n",
    "torch.full \t        filled with a given value\n",
    "torch.rand \t        i.i.d. continuous Uniform[0, 1)\n",
    "torch.randn \t    i.i.d. Normal(0, 1)\n",
    "torch.randint \t    i.i.d. discrete Uniform in given range\n",
    "torch.randperm \t    random permutation of {0, 1, ..., n - 1}\n",
    "torch.tensor \t    copied from existing data (list, NumPy ndarray, etc.)\n",
    "torch.from_numpy* \tfrom NumPy ndarray (sharing storage without copying)\n",
    "torch.arange,\n",
    "torch.range         uniformly spaced values in a given range\n",
    "torch.linspace\n",
    "torch.logspace \t    logarithmically spaced values in a given range\n",
    "torch.eye \t        identity matrix\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 10, 10])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(10, 10, 10, 10)\n",
    "\n",
    "# the indexing elements can have other shapes than 1\n",
    "b = a[[[3, 2]], :, [[1, 3]]]\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 10])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# broadcasting also supported in the indices, as well as lists,\n",
    "# negative indices, slices, elipses, numbers\n",
    "c = a[[1, -2], 2:4, :, [1]]\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10, 10])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can also support tensors as indices\n",
    "index = torch.tensor([2, 4])\n",
    "d = a[index]\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 10, 10, 10]), torch.Size([2, 10, 10, 10]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and the indices can be on the GPU or CPU\n",
    "e = a.cuda()[index.cuda()]\n",
    "f = a.cuda()[index.cpu()]\n",
    "e.shape, f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "indices should be either on cpu or on the same device as the indexed tensor (cpu)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/cosmos/Documents/Practices/ML-Practices/PyTorch/PyTorch Tricks/PyTorch_tricks.ipynb Cell 65\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/cosmos/Documents/Practices/ML-Practices/PyTorch/PyTorch%20Tricks/PyTorch_tricks.ipynb#Y124sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m g \u001b[39m=\u001b[39m a[index\u001b[39m.\u001b[39;49mcuda()]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: indices should be either on cpu or on the same device as the indexed tensor (cpu)"
     ]
    }
   ],
   "source": [
    "g = a[index.cuda()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 5, 10, 10])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.rand(10) > 0.5\n",
    "# we can now index with a mask that has fewer\n",
    "# dimensions than the indexing tensor\n",
    "c = a[mask, :5]\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add torch.reshape, which is similar to numpy.reshape. It is roughly equivalent to tensor.contiguous().view(), \n",
    "# but avoids copying in certain cases #5575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[0, 3, 6],\n",
      "        [1, 4, 7],\n",
      "        [2, 5, 8]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0, 9).reshape(3, 3)\n",
    "# the following transposes a\n",
    "b = torch.einsum('ij->ji', (a,))\n",
    "print(a, b, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add torch.expm1, a numerically stable exp(x)-1 for small x. #4350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add torch.where(condition, tensor1, tensor2) that returns a tensors of elements selected from tensor1 or tensor2 based on condition. #4259, #4259"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new autograd container that lets you trade compute for memory\n",
    "\n",
    "# The new checkpoint container allows you to only store a subset of the outputs necessary for backpropagation.\n",
    "# If an output is missing (to save memory), the checkpoint container will recompute the intermediate outputs from \n",
    "# the closest checkpoint, so that memory usage can be reduced (with an increase in computation time).\n",
    "# Here is an example:\n",
    "\n",
    "# create the input tensors and set the requires_grad=True\n",
    "# NOTE: the requires_grad=True for the input is a current\n",
    "# limitation of checkpointing. At least one of the \n",
    "# model inputs should have requires_grad=True. \n",
    "# If you don't do it, you might have empty gradients.\n",
    "input = torch.rand(1, 10, requires_grad=True)\n",
    "layers = [nn.Linear(10, 10) for _ in range(1000)]\n",
    "\n",
    "# define function that will define where\n",
    "# we will checkpoint and store\n",
    "# intermediate gradients. In this case,\n",
    "# we will only store one intermediate\n",
    "# gradient, in the middle of the\n",
    "# model\n",
    "\n",
    "def run_first_half(*args):\n",
    "    x = args[0]\n",
    "    for layer in layers[:500]:\n",
    "        x = layer(x)\n",
    "    return x\n",
    "\n",
    "def run_second_half(*args):\n",
    "    x = args[0]\n",
    "    for layer in layers[500:-1]:\n",
    "        x = layer(x)\n",
    "    return x\n",
    "\n",
    "# now uses the new checkpoint functionality\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "x = checkpoint(run_first_half, input)\n",
    "x = checkpoint(run_second_half, x)\n",
    "# last output need to be run without checkpoint\n",
    "x = layers[-1](x)\n",
    "x.sum.backward()  # works!\n",
    "\n",
    "# For sequential modules (which can have arbitrary blocks inside), a helper function checkpoint_sequential is provided, which takes care of the most common use-cases:\n",
    "\n",
    "input = torch.rand(1, 10, requires_grad=True)\n",
    "layers = [nn.Linear(10, 10) for _ in range(1000)]\n",
    "model = nn.Sequential(*layers)\n",
    "\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "\n",
    "# split in two blocks\n",
    "num_segments = 2\n",
    "x = checkpoint_sequential(model, num_segments, input)\n",
    "x.sum().backward()  # works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(2, 2), nn.ReLU(), nn.Linear(2, 2))\n",
    "del model[1]  # deletes nn.ReLU\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4, 7, 5, 0, 3]), tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randint(0, 8, (5,), dtype=torch.int64)\n",
    "weights = torch.linspace(0, 1, steps=5)\n",
    "input, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bincount(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7500, 0.0000, 0.0000, 1.0000, 0.0000, 0.5000, 0.0000, 0.2500])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.bincount(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.as_tensor (similar to torch.tensor but never copies unless necessary) #7109\n",
    "\n",
    "tensor = torch.randn(3, device='cpu', dtype=torch.float32)\n",
    "torch.as_tensor(tensor)                       # doesn't copy\n",
    "torch.as_tensor(tensor, dtype=torch.float64)  # copies due to incompatible dtype\n",
    "torch.as_tensor(tensor, device='cuda')        # copies due to incompatible device\n",
    "array = np.array([3, 4.5])\n",
    "torch.as_tensor(array)                        # doesn't copy, sharing memory with the numpy array\n",
    "torch.as_tensor(array, device='cuda')         # copies due to incompatible device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeakyReLU(negative_slope=0.01) 0.6847500801086426 ms\n",
      "PReLU(num_parameters=1) 0.8758091926574707 ms\n",
      "LeakyReLU(negative_slope=0.01) 0.6054043769836426 ms\n",
      "PReLU(num_parameters=1) 0.8743023872375488 ms\n",
      "LeakyReLU(negative_slope=0.01) 0.8593487739562988 ms\n",
      "PReLU(num_parameters=1) 0.8649063110351562 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS']='1'  #Use one CPU thread\n",
    "import torch, torch.nn as nn, time\n",
    "def test_net(net,offset):\n",
    "    net.eval()\n",
    "    total=0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(100):\n",
    "            x = torch.randn(100,100,100)+offset\n",
    "            start_time = time.time()\n",
    "            y = net(x)\n",
    "            total+=time.time()-start_time\n",
    "    print(net, total*10, 'ms')\n",
    "\n",
    "for offset in [-1,0,+1]:\n",
    "    test_net(nn.LeakyReLU(),offset) \n",
    "    test_net(nn.PReLU(),offset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "a.masked_select(torch.tensor([False,  True,  True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000],\n",
       "        [0.5000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        [0.0000],\n",
       "        [0.5000]], size=(10, 1), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=0.5, zero_point=8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch 1.3.0 release\n",
    "\n",
    "x = torch.rand(10,1, dtype=torch.float32)\n",
    "xq = torch.quantize_per_tensor(x, scale = 0.5, zero_point = 8, dtype=torch.quint8)\n",
    "xq\n",
    "# xq is a quantized tensor with data represented as quint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3489],\n",
       "        [0.4017],\n",
       "        [0.0223],\n",
       "        [0.1689],\n",
       "        [0.2939],\n",
       "        [0.5185],\n",
       "        [0.6977],\n",
       "        [0.8000],\n",
       "        [0.1610],\n",
       "        [0.2823]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdq = x.dequantize()\n",
    "xdq\n",
    "# convert back to floating point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6816, 0.9152, 0.3971, 0.8742, 0.4194],\n",
       "        [0.5529, 0.9527, 0.0362, 0.1852, 0.3734]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch 1.4.0 release\n",
    "\n",
    "from torch.nn.utils import prune\n",
    "\n",
    "t = torch.rand(2, 5)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.9152, 0.0000, 0.8742, 0.0000],\n",
       "        [0.0000, 0.9527, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = prune.L1Unstructured(amount=0.7)\n",
    "pruned_tensor = p.prune(t)\n",
    "pruned_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 1, kernel_size=(2, 2), stride=(1, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Conv2d(3, 1, 2)\n",
    "prune.ln_structured(module=m, name='weight', amount=0.5, n=2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.1\n",
      "1 0.09000000000000001\n",
      "2 0.08100000000000002\n",
      "3 0.007290000000000002\n",
      "4 0.006561000000000002\n"
     ]
    }
   ],
   "source": [
    "# LR Chaining\n",
    "\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR\n",
    "\n",
    "model = [torch.nn.Parameter(torch.randn(2, 2, requires_grad=True))]\n",
    "optimizer = SGD(model, 0.1)\n",
    "\n",
    "scheduler1 = ExponentialLR(optimizer, gamma=0.9)\n",
    "scheduler2 = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "for epoch in range(5):\n",
    "    print(epoch, scheduler2.get_last_lr()[0])\n",
    "    optimizer.step()\n",
    "    scheduler1.step()\n",
    "    scheduler2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = MyMod()\n",
    "torch.save(m.state_dict(), 'mymod.pt') # Saves a zipfile to mymod.pt\n",
    "\n",
    "# To use the old format, pass the flag _use_new_zipfile_serialization=False\n",
    "\n",
    "m = MyMod()\n",
    "torch.save(m.state_dict(), 'mymod.pt', _use_new_zipfile_serialization=False) # Saves pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 1.7.0 release\n",
    "\"\"\"\n",
    "[Beta] torch.set_deterministic\n",
    "\n",
    "Reproducibility (bit-for-bit determinism) may help identify errors when debugging or testing a program. To facilitate reproducibility, \n",
    "PyTorch 1.7 adds the torch.set_deterministic(bool) function that can direct PyTorch operators to select deterministic algorithms when available,\n",
    "and to throw a runtime error if an operation may result in nondeterministic behavior. By default, the flag this function controls is false and \n",
    "there is no change in behavior, meaning PyTorch may implement its operations nondeterministically by default.\n",
    "\n",
    "More precisely, when this flag is true:\n",
    "\n",
    "    Operations known to not have a deterministic implementation throw a runtime error;\n",
    "    Operations with deterministic variants use those variants (usually with a performance penalty versus the non-deterministic version); and\n",
    "    torch.backends.cudnn.deterministic = True is set.\n",
    "\n",
    "Note that this is necessary, but not sufficient, for determinism within a single run of a PyTorch program. Other sources of randomness like random \n",
    "number generators, unknown operations, or asynchronous or distributed computation may still cause nondeterministic behavior.\n",
    "\n",
    "See the documentation for torch.set_deterministic(bool) for the list of affected operations.\n",
    "\n",
    "    RFC | Link: https://github.com/pytorch/pytorch/issues/15359\n",
    "    Documentation | Link: https://pytorch.org/docs/stable/generated/torch.set_deterministic.html\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.utils' has no attribute 'collect_env'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/cosmos/Documents/Practices/ML-Practices/PyTorch/PyTorch Tricks/PyTorch_tricks.ipynb Cell 88\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/cosmos/Documents/Practices/ML-Practices/PyTorch/PyTorch%20Tricks/PyTorch_tricks.ipynb#Y153sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mcollect_env()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.utils' has no attribute 'collect_env'"
     ]
    }
   ],
   "source": [
    "torch.utils.collect_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_bf16_supported()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "th",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0eba0d3721dd99d5e85d92095d30b3f3baa213223e21c8471968ea015153b8df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
